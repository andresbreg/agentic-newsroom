"""
Crawler Service - Full Text Extraction using Crawl4AI
"""

import asyncio
from typing import List, Optional
from sqlalchemy.orm import Session
from models import NewsItem
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def crawl_news_items(db: Session, batch_size: int = 5):
    """
    Crawls news items that are in DISCOVERED status and missing full_content.
    """
    # 1. Fetch pending items
    items = db.query(NewsItem).filter(
        NewsItem.status == "DISCOVERED",
        NewsItem.full_content == None
    ).limit(20).all() # Process up to 20 per run

    if not items:
        print("[CRAWLER] No items pending full content extraction.")
        return

    print(f"[CRAWLER] Starting extraction for {len(items)} items...")

    # Configure crawler
    browser_config = BrowserConfig(
        headless=True,
        verbose=False
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=100, # Only keep content with > 100 words (Crawl4AI built-in)
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Process in batches
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            urls = [item.url for item in batch]
            
            print(f"[CRAWLER] Processing batch {i//batch_size + 1} ({len(urls)} URLs)...")
            
            try:
                # arun_many is optimized for multiple URLs
                results = await crawler.arun_many(
                    urls=urls,
                    config=run_config
                )

                for item, result in zip(batch, results):
                    if result.success:
                        content = result.markdown.raw_markdown if result.markdown else ""
                        
                        # Validate length (fallback to snippet if too short)
                        if content and len(content) > 150:
                            item.full_content = content
                            print(f"  [OK] Extracted {len(content)} chars from {item.url}")
                        else:
                            # Use content_snippet as fallback if extraction fails to get meaningful text
                            item.full_content = item.content_snippet
                            print(f"  [WARN] Content too short or empty for {item.url}, using snippet.")
                    else:
                        print(f"  [ERROR] Failed to crawl {item.url}: {result.error_message}")
                        # Even if it fails, we set it to snippet or empty to avoid re-processing forever
                        item.full_content = item.content_snippet or ""

                db.commit() # Commit after each batch
                
            except Exception as e:
                print(f"[CRAWLER] Batch error: {e}")
                db.rollback()

    print("[CRAWLER] Extraction process finished.")

def run_crawler_sync(db: Session, batch_size: int = 5):
    """Synchronous wrapper for async crawler."""
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
    
    loop.run_until_complete(crawl_news_items(db, batch_size))
